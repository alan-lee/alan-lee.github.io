clustering analysis就是将数据划分到不同的类里，使相似的数据在同一类里，不相似的数据在不同的类里。那么如何来衡量数据的相似性则是聚类算法的关键。

# K-Means
K-Means算法是最为经典的聚类算法之一。K-Means算法简单，效率高，很多算法都是围绕着K-Means算法来改造和扩展的。

假如我们要把数据$$\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$$划分为K个集群，其中，$$x^{(i)} \in R^{n}$$
K-Means算法的步骤如下：
1.在数据集中随机选取K个中心点, $$\mu_{1}, \mu_{2}, \cdots, \mu_{k}$$
2.对于数据集中的任一点$$x^{(i)}$$，计算其分类：
```math
c^{(i)} = \arg\min\limits_{j}\|x^{(i)} - \mu_{j}\|^{2} \quad  j = 1, 2, \cdots, k
```
3.重新计算每一个集群$$j$$的中心：
```math
\mu_{j} = \frac{\sum_{i=1}^{m}I(c^{(i)} = j)x^{(i)}}{\sum_{i=1}^{m}I(c^{(i)} = j)}
```
4.重复上述第2步，直到每个集群中的点不再变化，即算法收敛。

K-Means算法其实是想使每个点到集群中心的距离之和最小，定义失真函数：
```math
J(c, \mu) = \sum_{i=1}^{m}\|x^{(i)} - \mu_{c^{(i)}}\|^{2}
```
那么K-Means算法等价于求解$$\arg\min\limits_{\mu, c} J(c, \mu)$$，不管是第2步中的调整集群，还是第3步中调整中心，都是为了这个目标。

介绍完了K-Means算法后，还需要解释以下几个问题：
第一个问题：K-Means算法结束的条件是算法收敛，但是如何保证K-Means算法的收敛性？

证明K-Means算法是收敛很简单，我们可以证明迭代的每一个都可以优化$$J(c, \mu)$$，这样，$$J(c, \mu)$$不断减小，最后接近最小值，从而收敛。K-Means中的第2步使$$J(c, \mu)$$变小是很明显的，直接通过定义就可以看出。第3步的证明稍复杂，我们换个思路，当每个样本所属的集群已经固定了，即$$c$$是固定的，我们求解使$$J(c, \mu)$$取得最小值的$$\mu$$：令$$\frac{\partial}{\partial \mu}J(c, \mu) = 0$$即可得：$$\mu_{j} = \frac{\sum_{i=1}^{m}I(c^{(i)} = j)x^{(i)}}{\sum_{i=1}^{m}I(c^{(i)} = j)}$$。这表明，第3步中计算得出的$$\mu$$可以使$$J(c, \mu)$$取得最小。

第二个问题：在算法开始时，随机选取了$$K$$个点作为每个集群的中心，会不会因为选取的初始点不同导致结果不同？

答案是会，注意到失真函数$$J(c, \mu)$$并不是一个凸函数，也就是说，即便算法是收敛的，也有可能收敛到一个局部最优解，而不是全局最优解。在选取初始点时，可以多选几次，聚类后再比较$$J(c, \mu)$$，选取最小的。还有一种对K-Means算法的改进，[K-Means++算法](https://en.wikipedia.org/wiki/K-means%2B%2B)，这个算法的想法是在选取初始中心点时，使每个集群中心之间的相互距离尽可能的远。

第三个问题：集群的个数K如何确定？

有些时候，我们并不能很直观的看出集群的个数，通常的做法是多尝试几个K值，选择一个更合理的，更能解释相关业务的值。或是也采用$$J(c, \mu)$$来衡量K的选值。

最后说明以下，上述的K-Means算法的介绍中，我们是用欧氏距离来衡量两个点的相似性的，其实也可以采用其他的标准，例如：余弦相似性。

# Gaussian Mixture Model
和K-Means基于距离的算法不一样，混合高斯模型是一种基于分布模型的聚类算法。混合高斯模型考虑的是从样本集中拟合出数据的概率密度函数。假定样本集是有$$K$$个集群组成，每个集群都服从某种分布，混合高斯模型假定每个集群中都服从某个多元高斯分布。

首先回忆一下多元高斯分布，这个模型在介绍高斯辨别分析的时候提到过，多元高斯分布的概率密度函数为：
```math
p(x; \mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu))
```
其中，$$n$$是样本$$x$$的维数，$$\mu$$为多元高斯分布的期望，$$\Sigma$$为样本集的协方差，$$|\Sigma|$$表示为协方差的行列式，是一个标量。

由于高斯辨别分析算法是一种监督学习，所以样本中会给出所属的分类标签。这样，我们通过极大似然估计可以求解上面公式中的参数。

对于非监督学习来说，样本中并没有给出标签，首先我们要引入一个随机变量$$z$$，表示样本所属的集群，和K-Means一样，混合高斯模型算法需要我们事先给定集群数$$K$$，那么当$$K=2$$时，$$z$$服从伯努利分布，当$$K \gt 2$$时，$$z$$服从多项分布，即：
```math
z \sim Bernoulli(\phi), \quad K = 2
```
```math
z \sim Multinomial(\phi, K), \quad K \gt 2
```

那么，$$z$$就是先验分布，所有属于第$$i$$个集群的样本服从期望是$$\mu_{i}$$，协方差是$$\Sigma_{i}$$的多元高斯分布，即：
```math
x|z=i \sim N(\mu_{i}, \Sigma_{i})
```

根据全概率公式
```math
p(x) = \sum_{i=1}^{k}p(x|z=i)p(z=i)
```

似然函数：
```math
L(\phi, \mu, \Sigma) = \prod_{i=1}^{m}\sum_{j=1}^{k}p(x^{(i)}|z^{(i)}=j)p(z^{(i)}=j)
```
取对数：
```math
l(\phi, \mu, \Sigma) = \sum_{i=1}^{m}\log (\sum_{j=1}^{k}p(x^{(i)}|z^{(i)}=j)p(z^{(i)}=j))
```
遇到这种问题，一般的方法就是计算每个变量的梯度，然后用梯度上升算法来求解，注意观察上面的等式，在对数里还有求和，这使得求导后的式子变得非常复杂，甚至求的结果不是闭区间。针对这种情况，我们可以用期望最大算法来求解参数估计的问题。

期望最大算法分为两部：E-Step和M-Step。如果我们确定$$z^{(i)}$$的值，那么我们就可以用类似求解高斯辨别法的方法来求解混合高斯模型，所以在E-Step里，我们先随机初始化$$\phi, \mu, \Sigma$$，把$$\phi, \mu, \Sigma$$看成常量，先算出每个最有可能的$$z^{(i)}$$，在M-Step里，再根据$$z^{(i)}$$更新出$$\phi, \mu, \Sigma$$的值。然后再把算出的$$\phi, \mu, \Sigma$$代入到E-Step里，修正$$z^{(i)}$$，再用新的$$z^{(i)}$$在M-Step计算$$\phi, \mu, \Sigma$$。一直迭代到算法收敛为止。从描述上看，EM算法和坐标上升算法有些类似。用EM算法求解混合高斯模型的步骤如下：
###### E-Step
对于每一个$$i$$和$$j$$，计算：
```math
w_{j}^{(i)} = p(z^{(i)} = j | x^{(i)}; \phi, \mu, \Sigma)
```
```math
=\frac{p(x^{(i)} | z^{(i)} = j; \mu, \Sigma)p(z^{(i)} = j; \phi)}{p(x^{(i)};\mu, \Sigma)}
```
```math
=\frac{p(x^{(i)} | z^{(i)} = j; \mu, \Sigma)p(z^{(i)} = j; \phi)}{\sum_{l=1}^{k}p(x^{(i)} | z^{(i)} = l; \mu, \Sigma)p(z^{(i)} = l; \phi)}
```
###### M-Step
更新参数：
```math
\phi_{j} = \frac{1}{m}\sum_{i=1}^{m}w_{j}^{(i)}
```
```math
\mu_{j} = \frac{\sum_{i=1}^{m}w_{j}^{(i)}x^{(i)}}{\sum_{i=1}^{m}w_{j}^{(i)}}
```
```math
\Sigma_{j} = \frac{\sum_{i=1}^{m}w_{j}^{(i)}(x^{(i)} - \mu_{j})(x^{(i)} - \mu_{j})^{T}}{\sum_{i=1}^{m}w_{j}^{(i)}}
```
对比在高斯辨别分析算法中，我们计算出的$$\phi$$，$$\mu$$和$$\Sigma$$，可以看到，在EM算法中，我们用概率来代替了指示函数，这是因为样本中并没有给出标签，我们只能根据样本计算出$$z$$的后验概率。

# Expectation Maximization
下面介绍EM算法的一般形式和推导过程。
给定样本集$$\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}, x^{(i)} \in R^{n}$$，且样本间独立。我们的目标是将这个$$m$$个样本划分成$$k$$个集群，即找到每个样本属于的集群$$z$$，使得$$p(x, z;\theta)$$最大，其中$$\theta$$为参数。

首先还是要用到最大似然估计法：
```math
L(\theta) = \prod_{i=1}^{m}p(x^{(i)};\theta)
```
取对数：
```math
l(\theta) = \sum_{i=1}^{m}\log p(x^{(i)};\theta)
```
```math
=\sum_{i=1}^{m}\log \sum_{z^{(i)}}p(x^{(i)}, z^{(i)}; \theta)
```
在求解混合高斯模型时我们提到，这种对数里有求和的式子，我们很难用直接求导，令导数为0来求解最优化问题。首先，我们找到$$l(\theta)$$的一个下界$$J(\theta, z)$$，即$$l(\theta) \geqslant J(\theta, z)$$，然后调整参数来最大化$$J(\theta, z)$$，最后当$$J(\theta, z)$$取得最大值的时候，刚好也在$$l(\theta)$$的最大值上，这要求我们找到$$\theta, z$$使得$$J(\theta, z)$$取得最大，并且能满足上面不等于中取等号的条件。

在找$$l(\theta)$$下界时，我们需要用到Jensen不等式：
如果$$f$$时凸函数，$$X$$是随机变量，那么有：
```math
E[f(X)] \geqslant f(E[X])
```
特别的，如果$$f$$是严格凸函数，当且仅当$$X$$是常量时，上面式子中的等号成立。

当$$f$$是凹函数时，上面的不等耗方面相反。

这里强调一下凸函数的概念：令$$f$$是定义在实数域上的函数，如果对于所有的实数$$x$$，$$f(x)$$的二阶导数大于等于0，那么$$f$$就是凸函数。如果$$x$$是一个向量，那么当其hessian矩阵是半正定矩阵时，$$f$$是凸函数。如果$$f(x)$$的二阶导数严格大于0，那么$$f$$就是严格凸函数。

根据Jensen不等式：
```math
l(\theta) = \sum_{i=1}^{m}\log \sum_{z^{(i)}}p(x^{(i)}, z^{(i)}; \theta)
```
```math
= \sum_{i=1}^{m}\log \sum_{z^{(i)}}Q_{i}(z^{(i)})\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})}
```
```math
\geqslant \sum_{i=1}^{m}\sum_{z^{(i)}}Q_{i}(z^{(i)})\log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})}
```

因为对数函数是严格凹函数，如果我们把$$Q_{i}(z^{(i)})$$看成是$$z^{(i)}$$的某个分布的概率密度函数，那么$$\sum_{z^{(i)}}Q_{i}(z^{(i)})\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})}$$就是$$\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})}$$的期望。所以就可以得到$$l(\theta)$$的一个下界。

Jensen不等式中提到，当随机变量$$X$$是一个常量时，上面式子中的等号成立，即
当$$\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})} = c$$时，我们可以用右边来代替左边做计算。

下面讨论如何选取$$Q_{i}(z^{(i)})$$，它需要满足以下几个条件：
```math
\sum_{z^{(i)}}Q_{i}(z^{(i)}) = 1
```
```math
Q_{i}(z^{(i)}) \geqslant 0
```
```math
\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})} = c
```

这里我们选取
```math
Q_{i}(z^{(i)}) = \frac{p(x^{(i)}, z^{(i)}; \theta)}{\sum_{z}p(x^{(i)}, z; \theta)}
```
```math
= \frac{p(x^{(i)}, z^{(i)}; \theta)}{p(x^{(i)}; \theta)}
```
```math
= p(z^{(i)} | x^{(i)}; \theta)
```
这样有：
```math
\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})} = p(x^{(i)}; \theta)
```
是一个和$$z^{(i)}$$无关的值，可以看成是常量。

至此，我们可以求解EM算法中的问题了：
###### E-Step
把$$\theta$$看成常量，然后计算
```math
Q_{i}(z^{(i)}) = p(z^{(i)} | x^{(i)}; \theta)
```
###### M-Step
估算参数$$\theta$$：
```math
\theta = \arg\max\limits_{\theta} \sum_{i=1}^{m}\sum_{z^{(i)}}Q_{i}(z^{(i)})\log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_{i}(z^{(i)})}
```

循环迭代以上两步，直到算法收敛。
那么能不能保证EM算法的收敛性呢？
假设我们通过第$$t$$次迭代得到的$$\theta$$是$$\theta^{(t)}$$,然后在第$$t + 1$$次迭代的E-Step中，计算出$$Q_{i}^{(t)}(z^{(i)}) = p(z^{(i)} | x^{(i)}; \theta^{(t)})$$，那么
```math
l(\theta^{(t + 1)}) \geqslant \sum_{i=1}^{m}\sum_{z^{(i)}}Q^{(t)}_{i}(z^{(i)})\log \frac{p(x^{(i)}, z^{(i)}; \theta^{(t + 1)})}{Q^{(t)}_{i}(z^{(i)})}  
```
```math
\geqslant \sum_{i=1}^{m}\sum_{z^{(i)}}Q^{(t)}_{i}(z^{(i)})\log \frac{p(x^{(i)}, z^{(i)}; \theta^{(t)})}{Q^{(t)}_{i}(z^{(i)})}
```
```math
= l(\theta^{(t)})
```

解释一下上面的推导过程：第一个不等式是通过Jensen不等式得到，当然因为$$\frac{p(x^{(i)}, z^{(i)}; \theta^{(t + 1)})}{Q^{(t)}_{i}(z^{(i)})}$$不一定是常量，所以这里的等号不一定成立。第二个不等式是有M-Step的过程得到的，$$\theta^{(t + 1)}$$是通过第$$t + 1$$次迭代的M-Step得到的，而第$$t + 1$$次迭代的M-Step的优化目标就是$$ \sum_{i=1}^{m}\sum_{z^{(i)}}Q^{(t)}_{i}(z^{(i)})\log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q^{(t)}_{i}(z^{(i)})}$$，所以$$l(\theta^{(t + 1)})$$就是$$ \sum_{i=1}^{m}\sum_{z^{(i)}}Q^{(t)}_{i}(z^{(i)})\log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q^{(t)}_{i}(z^{(i)})}$$的最大值。

这样由归纳法可以得到，算法最终可以找到一个最大值（可能是局部最大值）,即算法是收敛的。


最后，我们再回过头看看K-Means和混合高斯模型：
混合高斯模型的E-Step其实就是在计算$$z^{(i)}$$的后验概率，M-Step的目标是
```math
\arg\max\limits_{\phi, \mu, \Sigma} \sum_{i=1}^{m}\sum_{j=1}^{k}w_{j}^{(i)}\log \frac{\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu))  \cdot \phi}{w_{j}^{(i)}}
```

令
```math
J(\phi, \mu, \Sigma) =  \sum_{i=1}^{m}\sum_{j=1}^{k}w_{j}^{(i)}\log \frac{\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}\exp(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu))  \cdot \phi}{w_{j}^{(i)}}
```

求解$$\bigtriangledown_{\mu} J(\phi, \mu, \Sigma)= 0$$和$$\bigtriangledown_{\Sigma} J(\phi, \mu, \Sigma)= 0$$可得$$\mu_{j} = \frac{\sum_{i=1}^{m}w_{j}^{(i)}x^{(i)}}{\sum_{i=1}^{m}w_{j}^{(i)}}$$和$$\Sigma_{j} = \frac{\sum_{i=1}^{m}w_{j}^{(i)}(x^{(i)} - \mu_{j})(x^{(i)} - \mu_{j})^{T}}{\sum_{i=1}^{m}w_{j}^{(i)}}$$

求解$$\phi$$的过程相对复杂，因为还有一个约束条件$$\sum_{i=1}^{k}\phi_{i} = 1$$，因此不能直接求导，只能通过拉格朗日乘数法来求解。
在去掉常数项后，我们优化的目标函数可以简化为$$J(\phi) = \sum_{i=1}^{m}\sum_{j=1}^{k}w_{j}^{(i)}\log \phi_{j}$$，构造拉格朗日乘数：
```math
L(\phi) = \sum_{i=1}^{m}\sum_{j=1}^{k}w_{j}^{(i)}\log \phi_{j} + \beta(\sum_{j=1}^{k}\phi_{j} - 1)
```
对$$\phi_{i}$$求偏导
```math
\frac{\partial }{\partial \phi_{j}}L(\phi) = \sum_{i=1}^{m}\frac{w_{j}^{(i)}}{\phi_{j}} + \beta
```

将偏导置0可得：$$\phi_{j} = -\frac{1}{\beta}\sum_{i=1}^{m}w_{j}^{(i)}$$，代入到$$\sum_{j=1}^{k}\phi_{j} = 1$$中，可得：
```math
\beta = - \sum_{i=1}^{m}\sum_{j=1}^{k}w_{j}^{(i)}
```
因为$$\sum_{j=1}^{k}w_{j}^{(i)} = 1$$，所以$$\beta=-m$$。

代入得：
```math
\phi_{j} = \frac{1}{m}\sum_{i=1}^{m}w_{j}^{(i)}
```

再看EM算法和K-Means的关系：
其实K-Means算法也是EM算法思想的体现。在E-Step中，我们计算每个样本属于某个集群的后验概率，而在K-Means算法中直接依据点之间的相似性给出了样本最可能在的集群。在M-Step中，我们通过极大似然估计来估算参数，而在K-Means算法中，是通过最小化点到集群中心的距离来计算出每个集群的中心。所以K-Means的第2步就是E-Step，第3步就是M-Step。
