在nn中，令L为总层数，用l表示层数的index，对于每一层，用i表示节点的index，对于每个节点记输入为$$z_{i}^{(l)}$$
输出为$$a_{i}^{(l)}$$
则有：
```math
a_{i}^{(1)} = z_{i}^{(1)}

a_{i}^{(l)} = g(z_{i}^{(l)}), l \neq 1
```
其中
```math
g(z) = sigmoid(z) = \frac{1}{1+e^{-z}}
```
叫做激励函数

令每一层的权重参数为$$W^{(l)}$$
如果l层有m个节点，l+1层有n个节点，那么权重参数是一个 n * (m+1)的矩阵，加1是因为在计算l层的输出的时候，会在输入的节点里加一个偏差节点，其值为1

$$w_{ij}^{(l)}$$表示第l层的第i个节点的输出对l+1层的第j个节点的输入的权重

则
```math
z^{(l+1)} = a^{(l)} * (W^{(l)})^{T}
```


![image](http://oirw7ycq0.bkt.clouddn.com/nn_intro.png)

如图有一个四层的neural netwrok，其中输入层有三个节点，有2个隐藏层，每个隐藏层有4个节点，最后的输出层有2个节点。下面分别讨论向前传播和向后传播的过程。这两个算法是nn在监督学习中最常用的算法。
# 向前传播

正向传播就是在已知输入和权重的情况下，来计算整个网络的输出
令输入为矩阵X，其中共有m个输入向量
对于每一个输入向量i = 1 : m, 令$$a^{(1)} = X[i, :]$$，
再计算第2到l层的每个节点的输入和输出，把向量化后可得：
```math
z^{(l)} = a^{(l - 1)} * (W^{(l - 1)})^{T},\, l > 1

a^{(l)} = g(z^{(l)}),\, l > 1
```

最后算出输出层节点的输出结果$$a^{L}$$即位整个网络的输出结果

![image](http://oirw7ycq0.bkt.clouddn.com/nn_fp.png)

如图，再向前传播的过程中，首先根据第一层的输入和权重计算出第二层的输入，再通过激励函数算出第二层的输出，然后依次算出每一层的输入和输出，最后得到输出层的输出。

# 向后传播

对于监督学习，我们学习的目的是使模型的输出和训练样本的目标的误差尽可能小，同时，我们的模型还要具有一定的普适性，防止过拟合的情况发生。对于nn，我们定义代价函数为
```math
J(W) = \frac{1}{m}\sum_{k=1}^{K}\sum_{i=1}^{m}[-y_{k}^{i}\log(h_{W}(x^{i}))_{k}) - (1 - -y_{k}^{i})\log(1 - h_{W}(x^{i}))_{k})]
```
```math
+ \frac{\lambda }{2m}[\sum_{l=1}^{L}\sum_{i=1}^{m^{l}}\sum_{j=1}^{n^{l}}(W_{ij}^{(l)})^{2}]
```

一般来说，因为我们的激励函数选用的是sigmoid函数，和logistic回归类似，这类的模型主要是用来处理分类问题，一般来说，对于类别的个数和输出节点的个数一致，我们通过向前传播计算出来的结果即为每种分类的概率，选择概率最大的作为最终的分类结果。

K为输出的节点的个数，m为训练样本的个数，加号前的一部分为计算误差，和逻辑回归的代价函数相比，这里就是把输出的K个节点的误差分别计算后再相加。加号后的一部分为正规化结果，这里选用L2范数，即所有权重参数的平方和，其中$$\lambda$$为正规化参数。

向后传播的思想就是通过将输出层的误差一层一层的向后传导，从而得到每一层的参数对最终误差的影响。最后通过最小化误差来计算每一层的参数。

在利用梯度下降或是拟牛顿法等算法来计算$$J(W)$$的最小值的时候
，我们都需要计算每个参数$$w^{(l)_{ij}}$$对于$$J(W)$$的梯度
$$\frac{\partial J(\theta)}{\partial w_{ij}^{(l)}}$$

下面就是推到$$\frac{\partial J(W)}{\partial w_{ij}^{(l)}}$$的过程：

对于样本中的任一样本，令其输出的为$$a^{(L)}$$(nn共有L层)，样本的目标结果为$$y$$

首先定义$$\delta^{(L)} = a^{(L)} - y$$， 其实$$\delta^{(L)}_{i}$$就是$$\frac{\partial J(W)}{\partial z_{i}^{(L)}}$$(不考虑正规化项)，推导如下：
```math
\frac{\partial J(W)}{\partial z_{i}^{(L)}} = \frac{1}{m}\frac{\partial }{\partial z_{i}^{(L)}} \sum_{k=1}^{K}\sum_{j=1}^{m}[-y_{k}^{i}\log(g(z^{L}_{i}))_{k}) - (1 - -y_{k}^{i})\log(1 - g(z^{(L)}_{i}))_{k}]
```
```math
=[-\frac{y^{i}}{g(z^{(L)}_{i})} - \frac{(1 - -y^{i})}{1 - g(z^{(L)}_{i})}] * g{}'(z^{(L)}_{i})
```
```math
= [-\frac{y^{i}}{g(z^{(L)}_{i})} - \frac{(1 - -y^{i})}{1 - g(z^{(L)}_{i})}] * g(z^{(L)}_{i}) * (1 - g(z^{(L)}_{i}))
```
```math
= -y^{i} * (1 - g(z^{(L)}_{i})) -  (1 - -y^{i}) * g(z^{(L)}_{i})
```
```math
=  g(z^{(L)}_{i}) - y^{i} = a^{(L)}_{i} - y_{i}
```

因为$$z^{(L)}_{i}$$是一个he和K无关的变量，因此在求导数的过程中，可以直接把$$\sum_{k=1}^{K}$$去掉，m为样本数量，对于每个样本来说，$$z^{(L)}_{i}$$都是一样的变量，因此这里可以和前面的$$\frac{1}{m}$$约去。sigmoid函数的倒数也是求导过程中的关键点：

```math
g{}'(z^{L}_{i}) = g(z^{L}_{i}) * (1 - g(z^{L}_{i}))
```

令
```math
\delta^{(l)} = \frac{\partial J(W)}{\partial z_{i}^{(l)}}

= \frac{\partial J(W)}{\partial a_{i}^{(l)}} * \frac{\partial a_{i}^{(l)}}{\partial z_{i}^{(l)}}

= (\sum_{j=1}^{n^{(l+1)}}\frac{\partial J(W)}{\partial z_{j}^{(l+1)}} * \frac{\partial z_{j}^{(l+1)}}{\partial a_{i}^{(l)}}) *  \frac{\partial a_{i}^{(l)}}{\partial z_{i}^{(l)}}

= \sum_{j=1}^{n^{(l+1)}}(\delta_{j}^{(l+1)} * w_{ij}^{(l)}) * g{}'(z_{i}^{(l)})

= \sum_{j=1}^{n^{(l+1)}}(\delta_{j}^{(l+1)} * w_{ij}^{(l)}) * g(z_{i}^{(l)}) * (1 - g(z_{i}^{(l)}))

= \sum_{j=1}^{n^{(l+1)}}(\delta_{j}^{(l+1)} * w_{ij}^{(l)}) * a_{i}^{(l)} * (1 - a_{i}^{(l)})
```

向量化后可得
```math
\delta^{(l)} = (W^{(l)})^{T} * \delta^{(l + 1)} .* a_{i}^{(l)} .* (1 - a_{i}^{(l)})
```

这就是反向传播的过程，即通过$$\delta^{(l+1)}$$来计算出$$\delta^{(l)}$$

在推导过程中，用到了导数的链式法则，即
```math
g{}'(f(x)) = g{}'(z) * f{}'(x), z = f(x)

\frac{\partial g(x, y)}{\partial x} = \frac{\partial g(u)}{\partial u} * \frac{\partial u(x, y)}{\partial x} + \frac{\partial g(v)}{\partial v} * \frac{\partial v(x, y)}{\partial x}
```

在计算$$\frac{\partial J(W)}{\partial a_{i}^{(l)}}$$时，因为$$a_{i}^{(l)}$$参与了l+1层的每个节点的输入的计算，所以在用链式法则展开时，必须把每个节点的计算过程都考虑进去。

需要注意：
```math
\frac{\partial a_{i}^{(l)}}{\partial z_{i}^{(l)}} = g{}'(z_{i}^{(l)}) = g(z_{i}^{(l)})(1 - g(z_{i}^{(l)})) = a_{i}^{(l)}(1 - a_{i}^{(l)})

\frac{\partial z_{j}^{(l+1)}}{\partial a_{i}^{(l)}} =  \frac{\partial}{\partial a_{i}^{(l)}} (w^{(l)}_{j})^{T} a^{(l)} = w^{(l)}_{ij}
```
最后由$$\delta^{(l)}_{i}$$推导出$$\frac{\partial z_{j}^{(l+1)}}{\partial w_{ij}^{(l)}}$$

```math
\frac{\partial J(W)}{\partial w_{ij}^{(l)}} =
\frac{\partial J(W)}{\partial z_{j}^{(l+1)}} * \frac{\partial z_{j}^{(l+1)}}{\partial w_{ij}^{(l)}} = \delta^{(l+1)}_{j} * a^{(l)}_{i}
```

加上正规化项后，有

```math
\frac{\partial J(W)}{\partial w_{ij}^{(l)}} =  \delta^{(l+1)}_{j} * a^{(l)}_{i} + \frac{1}{m}w_{ij}^{(l)}, j \neq 0

\frac{\partial J(W)}{\partial w_{ij}^{(l)}} =  \delta^{(l+1)}_{j} * a^{(l)}_{i} , j = 0
```

整个过程如下，
令所令有参数的梯度$$D^{l}_{ij} = 0$$，对于每一个输入向量i = 1 : m, 令$$a^{(1)} = X[i, :]$$, 先通过正向传播计算出$$a^{(l)}$$,$$z^{(l)}$$，然后计算

```math
\delta^{(L)} = y - a^{(L)}

\delta^{(l)} = (W^{(l)})^{T} * \delta^{(l+1)} .* a^{(l)} .* (1 - a^{(l)})

D^{(l)} = D^{(l)} + \delta^{(l+1)}[2:] * (a^{(l)})^{T}
```

最后当所有样本都计算完成后，


```math
D^{(l)}_{ij} =  \frac{1}{m}(D^{l}_{ij} * a^{(l)}_{i} + w_{ij}^{(l)}), j \neq 0

D^{(l)}_{ij} =  \frac{1}{m}(D^{(l)}_{ij}) , j = 0
```

在计算$$D^{(l)}$$时，取的是$$\delta^{(l+1)}[2:]$$，这是因为
每一层的偏差节点默认都是1，并不是由前面的节点计算得出，因此不参与向后传播的计算。

最后，在用梯度下降或者你牛顿法计算$$W^{(l)}$$时，还需要一个输入一个初始化的值，如果我们把$$W^{(l)}$$初始化为一个全0的矩阵或是让每个$$w^{(l)}_{ij}$$都相等，很容易证明，我们计算出的最终结果每层的$$w^{(l)}_{ij}$$也都相等，显然这是不对的，因此我们要随机化初始的参数。

设函数$$rand()$$返回一个$$[0, 1]$$内的随机数，那么为了让$$w^{(l)}_{ij}$$在$$[-\epsilon, \epsilon]$$的范围内，有：

```math
w^{(l)}_{ij} = rand() * 2 * \epsilon - \epsilon
```
