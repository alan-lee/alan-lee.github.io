clustering analyze就是将数据划分到不同的类里，使相似的数据在同一类里，不相似的数据在不同的类里。那么如何来衡量数据的相似性则是聚类算法的关键。

# K-Means
K-Means算法是最为经典的聚类算法之一。K-Means算法简单，效率高，很多算法都是围绕着K-Means算法来改造和扩展的。

假如我们要把数据$$\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$$划分为K个集群，其中，$$x^{(i)} \in R^{n}$$
K-Means算法的步骤如下：
1. 在数据集中随机选取K个中心点, $$\mu_{1}, \mu_{2}, \cdots, \mu_{k}$$
2. 对于数据集中的任一点$$x^{(i)}$$，计算其分类：
```math
c^{(i)} = \arg\min\limits_{j}\|x^{(i)} - \mu_{j}\|^{2} \quad  j = 1, 2, \cdots, k
```
3. 重新计算每一个集群$$j$$的中心：
```math
\mu_{j} = \frac{\sum_{i=1}^{m}I(c^{(i)} = j)x^{(i)}}{\sum_{i=1}^{m}I(c^{(i)} = j)}
```
4. 重复上述第2步，直到每个集群中的点不再变化，即算法收敛。

第一个问题：K-Means算法结束的条件是算法收敛，但是如果保证K-Means算法的收敛性？在后面介绍完EM算法(期望最大算法)后，会讨论K-Means算法的收敛性。

第二个问题：在算法开始时，随机选取了$$K$$个点作为每个集群的中心，会不会因为选取的初始点不同导致结果不同？答案是会，注意到，K-Means算法其实是想使每个点到集群中心的距离之和最小，即$$\arg\min\limits_{\mu, c} \sum_{i=1}^{m}\|x^{(i)} - \mu_{c^{(i)}}\|^{2}$$，不管是第2步中的调整集群，还是第3步中调整中心，都是为了这个目标。但是这个函数并不是一个凸函数，也就是说，即便算法是收敛的，也有可能收敛到一个局部最优解，而不是全局最优解。在选取初始点时，可以多选几次，聚类后再比较$$\sum_{i=1}^{m}\|x^{(i)} - \mu_{c^{(i)}}\|^{2}$$，选取最小的。还有一种对K-Means算法的改进，[K-Means++算法](https://en.wikipedia.org/wiki/K-means%2B%2B)，这个算法的想法是在选取初始中心点时，使每个集群中心之间的相互距离尽可能的远。

第三个问题：集群的个数K如何确定？有些时候，我们并不能很直观的看出集群的个数，通常的做法是多尝试几个K值，选择一个更合理的，更能解释相关业务的值。或是也采用$$\sum_{i=1}^{m}\|x^{(i)} - \mu_{c^{(i)}}\|^{2}$$的大小来衡量K的选值。

最后说明一下，上述K-Means算法的步骤中，我们在计算两个点的距离采用的是欧氏距离公式，
# Gaussian Mixture Model
